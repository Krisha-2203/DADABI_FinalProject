{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48d48354-d301-483f-9134-bee3dffc9ca9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bronze Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130dcc99-1bc8-4f40-a2b2-3b7b79806da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from workspace.imdb_final_project.bronze_name_basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37480bc1-5176-4c9e-93a2-3f67901a4402",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from workspace.imdb_final_project.bronze_title_akas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42325839-424a-461c-b794-2bb4c15a9b77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from workspace.imdb_final_project.bronze_title_basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f43095a-3c04-42ef-99b7-637df98a3d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.bronze_title_crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d5387d6-69cf-4813-8828-c5f98b808da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.bronze_title_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f11a5a6-fc67-45e9-ba76-d91931682887",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.bronze_title_principals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a6665d8-6bdd-418c-b7d5-de3490850098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.bronze_title_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4986f45c-fbfc-4eda-9fc1-9ac18644a87c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Row count for bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7d26fe-f3e2-4b36-85ac-07418c1df7df",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"table_name\":172},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764999389813}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- All Row Counts in Single Query\n",
    "SELECT 'bronze_name_basics' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.bronze_name_basics\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'bronze_title_akas' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.bronze_title_akas\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'bronze_title_basics' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.bronze_title_basics\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'bronze_title_crew' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.bronze_title_crew\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'bronze_title_episode' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.bronze_title_episode\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'bronze_title_principals' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.bronze_title_principals\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'bronze_title_ratings' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.bronze_title_ratings\n",
    "\n",
    "ORDER BY table_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6813336c-1a68-4566-94f2-ec795510e696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Silver Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621edd63-6cc8-4066-a645-4d9595625a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- All Row Counts in Single Query - Silver Tables\n",
    "SELECT 'silver_name_basics' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.silver_name_basics\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'silver_title_akas' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.silver_title_akas\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'silver_title_basics' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.silver_title_basics\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'silver_title_crew' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.silver_title_crew\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'silver_title_episode' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.silver_title_episode\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'silver_title_principals' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.silver_title_principals\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT 'silver_title_ratings' as table_name, COUNT(*) as row_count\n",
    "FROM imdb_final_project.silver_title_ratings\n",
    "\n",
    "ORDER BY table_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73936aa5-7f53-451d-9135-1cc29ffa9941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b02b692-2952-4c3d-812d-efedfdde6faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_bridge_akas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34df9def-7fce-4482-b599-953052fee267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_bridge_profession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61da46ce-aeb2-4f4e-9786-ebf0303c59a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_bridge_title_crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b054f856-6629-4074-ae0e-b703cc253bfd",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765061546275}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_bridge_title_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec849c2-3fd5-4bba-bb5c-ab96813300db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_dim_crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96bd08ea-e69d-4ea2-b256-acf305d334c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_dim_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d358e1a-088f-48c5-aaa4-8d11a4eeed27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_dim_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b1b077d-8db5-4b02-9c3e-f9624742d3a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_dim_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c4e7ca9-5b36-4cfa-88b5-972f487c70b2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ModifiedDate\":245},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765132659505}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_dim_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d84369-a973-4e86-8d06-2439b6b63d8c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"ModifiedDate\":201},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765132694369}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_dim_principals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef7935f-a71d-454e-89be-a1a0e066c89d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_dim_profession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b59b5ae-c372-4143-b8b7-ef294e49e54d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765061253213}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_dim_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e9ce972-c64b-4125-ab66-3f364760479d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Fact Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07ea1b8-de34-4ab7-a04a-71e7d90a3ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_fact_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d39a1a-9f99-4420-ae31-1239ae05868d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from imdb_final_project.gold_fact_title_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b7eaee7-199c-4774-b1c2-f0fd9ba1beed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Null Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adda7f64-81a7-49a6-ad8e-91b4f5732102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Gold Layer - Null Analysis for All Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, count, when, isnan, sum as _sum, lit\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "def analyze_nulls(df, table_name):\n",
    "    \"\"\"\n",
    "    Comprehensive null analysis for a DataFrame\n",
    "    Returns a summary DataFrame with null counts and percentages\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    null_counts = []\n",
    "    \n",
    "    for column in df.columns:\n",
    "        col_type = df.schema[column].dataType\n",
    "        \n",
    "        # Count NULLs\n",
    "        null_count = df.filter(col(column).isNull()).count()\n",
    "        \n",
    "        # For numeric columns, also check for NaN\n",
    "        if isinstance(col_type, (DoubleType, FloatType)):\n",
    "            nan_count = df.filter(isnan(col(column))).count()\n",
    "        else:\n",
    "            nan_count = 0\n",
    "        \n",
    "        # Count empty strings for string columns\n",
    "        if col_type.typeName() == 'string':\n",
    "            empty_count = df.filter((col(column) == \"\") | (col(column) == \"\\\\N\")).count()\n",
    "        else:\n",
    "            empty_count = 0\n",
    "        \n",
    "        total_nullish = null_count + nan_count + empty_count\n",
    "        null_percentage = (total_nullish / total_rows * 100) if total_rows > 0 else 0\n",
    "        \n",
    "        null_counts.append({\n",
    "            'table_name': table_name,\n",
    "            'column_name': column,\n",
    "            'total_rows': total_rows,\n",
    "            'null_count': null_count,\n",
    "            'nan_count': nan_count,\n",
    "            'empty_string_count': empty_count,\n",
    "            'total_nullish': total_nullish,\n",
    "            'null_percentage': round(null_percentage, 2)\n",
    "        })\n",
    "    \n",
    "    return spark.createDataFrame(null_counts)\n",
    "\n",
    "print(\"Null analysis function loaded\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Dimension Tables Null Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 1. DIM_Region\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_DIM_Region\")\n",
    "print(\"=\" * 80)\n",
    "dim_region = spark.read.table(\"imdb_final_project.gold_DIM_Region\")\n",
    "region_nulls = analyze_nulls(dim_region, \"gold_DIM_Region\")\n",
    "region_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 2. DIM_Language\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_DIM_Language\")\n",
    "print(\"=\" * 80)\n",
    "dim_language = spark.read.table(\"imdb_final_project.gold_DIM_Language\")\n",
    "language_nulls = analyze_nulls(dim_language, \"gold_DIM_Language\")\n",
    "language_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 3. DIM_NAME\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_DIM_NAME\")\n",
    "print(\"=\" * 80)\n",
    "dim_name = spark.read.table(\"imdb_final_project.gold_DIM_NAME\")\n",
    "name_nulls = analyze_nulls(dim_name, \"gold_DIM_NAME\")\n",
    "name_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 4. DIM_Title\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_DIM_Title\")\n",
    "print(\"=\" * 80)\n",
    "dim_title = spark.read.table(\"imdb_final_project.gold_DIM_Title\")\n",
    "title_nulls = analyze_nulls(dim_title, \"gold_DIM_Title\")\n",
    "title_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 5. DIM_Genre\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_DIM_Genre\")\n",
    "print(\"=\" * 80)\n",
    "dim_genre = spark.read.table(\"imdb_final_project.gold_DIM_Genre\")\n",
    "genre_nulls = analyze_nulls(dim_genre, \"gold_DIM_Genre\")\n",
    "genre_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 6. DIM_Profession\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_DIM_Profession\")\n",
    "print(\"=\" * 80)\n",
    "dim_profession = spark.read.table(\"imdb_final_project.gold_DIM_Profession\")\n",
    "profession_nulls = analyze_nulls(dim_profession, \"gold_DIM_Profession\")\n",
    "profession_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 7. DIM_Crew\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_DIM_Crew\")\n",
    "print(\"=\" * 80)\n",
    "dim_crew = spark.read.table(\"imdb_final_project.gold_DIM_Crew\")\n",
    "crew_nulls = analyze_nulls(dim_crew, \"gold_DIM_Crew\")\n",
    "crew_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 8. DIM_Principals\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_DIM_Principals\")\n",
    "print(\"=\" * 80)\n",
    "dim_principals = spark.read.table(\"imdb_final_project.gold_DIM_Principals\")\n",
    "principals_nulls = analyze_nulls(dim_principals, \"gold_DIM_Principals\")\n",
    "principals_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Fact Tables Null Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 9. FACT_Title_Ratings\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_FACT_Title_Ratings\")\n",
    "print(\"=\" * 80)\n",
    "fact_ratings = spark.read.table(\"imdb_final_project.gold_FACT_Title_Ratings\")\n",
    "ratings_nulls = analyze_nulls(fact_ratings, \"gold_FACT_Title_Ratings\")\n",
    "ratings_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 10. FACT_Episodes\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_FACT_Episodes\")\n",
    "print(\"=\" * 80)\n",
    "fact_episodes = spark.read.table(\"imdb_final_project.gold_FACT_Episodes\")\n",
    "episodes_nulls = analyze_nulls(fact_episodes, \"gold_FACT_Episodes\")\n",
    "episodes_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Bridge Tables Null Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 11. BRIDGE_TITLE_GENRE\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_BRIDGE_TITLE_GENRE\")\n",
    "print(\"=\" * 80)\n",
    "bridge_genre = spark.read.table(\"imdb_final_project.gold_BRIDGE_TITLE_GENRE\")\n",
    "bridge_genre_nulls = analyze_nulls(bridge_genre, \"gold_BRIDGE_TITLE_GENRE\")\n",
    "bridge_genre_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 12. BRIDGE_PROFESSION\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_BRIDGE_PROFESSION\")\n",
    "print(\"=\" * 80)\n",
    "bridge_profession = spark.read.table(\"imdb_final_project.gold_BRIDGE_PROFESSION\")\n",
    "bridge_profession_nulls = analyze_nulls(bridge_profession, \"gold_BRIDGE_PROFESSION\")\n",
    "bridge_profession_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 13. Bridge_Title_Crew\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_Bridge_Title_Crew\")\n",
    "print(\"=\" * 80)\n",
    "bridge_crew = spark.read.table(\"imdb_final_project.gold_Bridge_Title_Crew\")\n",
    "bridge_crew_nulls = analyze_nulls(bridge_crew, \"gold_Bridge_Title_Crew\")\n",
    "bridge_crew_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 14. BRIDGE_Akas\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYZING: gold_BRIDGE_Akas\")\n",
    "print(\"=\" * 80)\n",
    "bridge_akas = spark.read.table(\"imdb_final_project.gold_BRIDGE_Akas\")\n",
    "bridge_akas_nulls = analyze_nulls(bridge_akas, \"gold_BRIDGE_Akas\")\n",
    "bridge_akas_nulls.show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Consolidated Null Summary - All Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONSOLIDATED NULL ANALYSIS - ALL GOLD TABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine all null analyses\n",
    "all_nulls = (\n",
    "    region_nulls\n",
    "    .union(language_nulls)\n",
    "    .union(name_nulls)\n",
    "    .union(title_nulls)\n",
    "    .union(genre_nulls)\n",
    "    .union(profession_nulls)\n",
    "    .union(crew_nulls)\n",
    "    .union(principals_nulls)\n",
    "    .union(ratings_nulls)\n",
    "    .union(episodes_nulls)\n",
    "    .union(bridge_genre_nulls)\n",
    "    .union(bridge_profession_nulls)\n",
    "    .union(bridge_crew_nulls)\n",
    "    .union(bridge_akas_nulls)\n",
    ")\n",
    "\n",
    "# Show only columns with nulls\n",
    "columns_with_nulls = all_nulls.filter(col(\"total_nullish\") > 0).orderBy(col(\"null_percentage\").desc())\n",
    "\n",
    "print(\"\\nüìä COLUMNS WITH NULL VALUES (sorted by null percentage):\")\n",
    "columns_with_nulls.show(100, truncate=False)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nüìà SUMMARY STATISTICS:\")\n",
    "all_nulls.groupBy(\"table_name\").agg(\n",
    "    count(\"*\").alias(\"total_columns\"),\n",
    "    _sum(when(col(\"total_nullish\") > 0, 1).otherwise(0)).alias(\"columns_with_nulls\"),\n",
    "    _sum(\"total_nullish\").alias(\"total_null_values\")\n",
    ").orderBy(\"columns_with_nulls\", ascending=False).show(truncate=False)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Row Count Summary - All Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ROW COUNT SUMMARY - ALL GOLD TABLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "table_counts = [\n",
    "    {\"table_name\": \"gold_DIM_Region\", \"row_count\": dim_region.count()},\n",
    "    {\"table_name\": \"gold_DIM_Language\", \"row_count\": dim_language.count()},\n",
    "    {\"table_name\": \"gold_DIM_NAME\", \"row_count\": dim_name.count()},\n",
    "    {\"table_name\": \"gold_DIM_Title\", \"row_count\": dim_title.count()},\n",
    "    {\"table_name\": \"gold_DIM_Genre\", \"row_count\": dim_genre.count()},\n",
    "    {\"table_name\": \"gold_DIM_Profession\", \"row_count\": dim_profession.count()},\n",
    "    {\"table_name\": \"gold_DIM_Crew\", \"row_count\": dim_crew.count()},\n",
    "    {\"table_name\": \"gold_DIM_Principals\", \"row_count\": dim_principals.count()},\n",
    "    {\"table_name\": \"gold_FACT_Title_Ratings\", \"row_count\": fact_ratings.count()},\n",
    "    {\"table_name\": \"gold_FACT_Episodes\", \"row_count\": fact_episodes.count()},\n",
    "    {\"table_name\": \"gold_BRIDGE_TITLE_GENRE\", \"row_count\": bridge_genre.count()},\n",
    "    {\"table_name\": \"gold_BRIDGE_PROFESSION\", \"row_count\": bridge_profession.count()},\n",
    "    {\"table_name\": \"gold_Bridge_Title_Crew\", \"row_count\": bridge_crew.count()},\n",
    "    {\"table_name\": \"gold_BRIDGE_Akas\", \"row_count\": bridge_akas.count()}\n",
    "]\n",
    "\n",
    "counts_df = spark.createDataFrame(table_counts).orderBy(col(\"row_count\").desc())\n",
    "counts_df.show(truncate=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Total tables analyzed: {len(table_counts)}\")\n",
    "print(f\"‚ö†Ô∏è  Empty tables: {counts_df.filter(col('row_count') == 0).count()}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Quick Validation Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"QUICK VALIDATION CHECKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for empty tables\n",
    "empty_tables = [t['table_name'] for t in table_counts if t['row_count'] == 0]\n",
    "if empty_tables:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Empty tables found: {', '.join(empty_tables)}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All tables have data\")\n",
    "\n",
    "# Check for high null percentages (>50%)\n",
    "high_null_columns = columns_with_nulls.filter(col(\"null_percentage\") > 50)\n",
    "high_null_count = high_null_columns.count()\n",
    "\n",
    "if high_null_count > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: {high_null_count} columns have >50% null values:\")\n",
    "    high_null_columns.select(\"table_name\", \"column_name\", \"null_percentage\").show(truncate=False)\n",
    "else:\n",
    "    print(\"\\n‚úÖ No columns with >50% null values\")\n",
    "\n",
    "# Check dimension table integrity\n",
    "print(\"\\nüìä DIMENSION TABLE INTEGRITY:\")\n",
    "print(f\"   Regions: {dim_region.count()} records\")\n",
    "print(f\"   Languages: {dim_language.count()} records\")\n",
    "print(f\"   Names: {dim_name.count()} records\")\n",
    "print(f\"   Titles: {dim_title.count()} records\")\n",
    "print(f\"   Genres: {dim_genre.count()} records\")\n",
    "print(f\"   Professions: {dim_profession.count()} records\")\n",
    "\n",
    "print(\"\\n‚úÖ Null analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4701eba-77aa-4f2c-9d78-8a54ad68c530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Duplicate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af44edd-a525-4e24-ab20-9a6c49799b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Gold Layer - Duplicate Analysis for All Tables\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, count, sum as _sum, desc, lit\n",
    "from pyspark.sql import Window\n",
    "\n",
    "def analyze_duplicates(df, table_name, key_columns, description=\"\"):\n",
    "    \"\"\"\n",
    "    Comprehensive duplicate analysis for a DataFrame\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to analyze\n",
    "    - table_name: Name of the table\n",
    "    - key_columns: List of columns that should be unique (business key)\n",
    "    - description: Optional description of what makes a record unique\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    if total_rows == 0:\n",
    "        print(f\"‚ö†Ô∏è  {table_name} is EMPTY - skipping duplicate analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Count duplicates based on key columns\n",
    "    duplicate_check = (\n",
    "        df.groupBy(key_columns)\n",
    "        .agg(count(\"*\").alias(\"occurrence_count\"))\n",
    "        .filter(col(\"occurrence_count\") > 1)\n",
    "    )\n",
    "    \n",
    "    duplicate_count = duplicate_check.count()\n",
    "    total_duplicate_rows = duplicate_check.agg(_sum(\"occurrence_count\")).collect()[0][0]\n",
    "    \n",
    "    if total_duplicate_rows:\n",
    "        total_duplicate_rows = int(total_duplicate_rows)\n",
    "    else:\n",
    "        total_duplicate_rows = 0\n",
    "    \n",
    "    duplicate_percentage = (total_duplicate_rows / total_rows * 100) if total_rows > 0 else 0\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(f\"TABLE: {table_name}\")\n",
    "    print(f\"DESCRIPTION: {description}\")\n",
    "    print(f\"KEY COLUMNS: {', '.join(key_columns)}\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Total Rows: {total_rows:,}\")\n",
    "    print(f\"Unique Key Combinations: {total_rows - total_duplicate_rows + duplicate_count:,}\")\n",
    "    print(f\"Duplicate Key Combinations: {duplicate_count:,}\")\n",
    "    print(f\"Total Duplicate Rows: {total_duplicate_rows:,}\")\n",
    "    print(f\"Duplicate Percentage: {duplicate_percentage:.2f}%\")\n",
    "    \n",
    "    if duplicate_count > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: {duplicate_count} duplicate key combinations found!\")\n",
    "        print(\"\\nTop 10 duplicates:\")\n",
    "        duplicate_check.orderBy(desc(\"occurrence_count\")).show(10, truncate=False)\n",
    "        \n",
    "        # Show sample duplicate records\n",
    "        print(\"\\nSample duplicate records:\")\n",
    "        duplicate_keys = duplicate_check.limit(3)\n",
    "        for row in duplicate_keys.collect():\n",
    "            key_values = {k: row[k] for k in key_columns}\n",
    "            filter_condition = None\n",
    "            for k, v in key_values.items():\n",
    "                condition = col(k) == v\n",
    "                filter_condition = condition if filter_condition is None else filter_condition & condition\n",
    "            \n",
    "            print(f\"\\n  Duplicate set: {key_values}\")\n",
    "            df.filter(filter_condition).show(5, truncate=False)\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No duplicates found - all records are unique!\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"table_name\": table_name,\n",
    "        \"total_rows\": total_rows,\n",
    "        \"duplicate_count\": duplicate_count,\n",
    "        \"total_duplicate_rows\": total_duplicate_rows,\n",
    "        \"duplicate_percentage\": round(duplicate_percentage, 2)\n",
    "    }\n",
    "\n",
    "print(\"Duplicate analysis function loaded\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Dimension Tables Duplicate Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 1. DIM_Region\n",
    "dim_region = spark.read.table(\"imdb_final_project.gold_DIM_Region\")\n",
    "region_dups = analyze_duplicates(\n",
    "    dim_region, \n",
    "    \"gold_DIM_Region\", \n",
    "    [\"RegionKey\"],\n",
    "    \"Each region should have unique RegionKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "# Also check business key\n",
    "region_dups_business = analyze_duplicates(\n",
    "    dim_region, \n",
    "    \"gold_DIM_Region (Business Key)\", \n",
    "    [\"RegionCode\"],\n",
    "    \"Each region should have unique RegionCode (business key)\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 2. DIM_Language\n",
    "dim_language = spark.read.table(\"imdb_final_project.gold_DIM_Language\")\n",
    "language_dups = analyze_duplicates(\n",
    "    dim_language, \n",
    "    \"gold_DIM_Language\", \n",
    "    [\"LanguageKey\"],\n",
    "    \"Each language should have unique LanguageKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "language_dups_business = analyze_duplicates(\n",
    "    dim_language, \n",
    "    \"gold_DIM_Language (Business Key)\", \n",
    "    [\"LanguageCode\"],\n",
    "    \"Each language should have unique LanguageCode (business key)\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 3. DIM_NAME\n",
    "dim_name = spark.read.table(\"imdb_final_project.gold_DIM_NAME\")\n",
    "name_dups = analyze_duplicates(\n",
    "    dim_name, \n",
    "    \"gold_DIM_NAME\", \n",
    "    [\"NameKey\"],\n",
    "    \"Each person should have unique NameKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "name_dups_business = analyze_duplicates(\n",
    "    dim_name, \n",
    "    \"gold_DIM_NAME (Business Key)\", \n",
    "    [\"NCONST\"],\n",
    "    \"Each person should have unique NCONST (business key)\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 4. DIM_Title\n",
    "dim_title = spark.read.table(\"imdb_final_project.gold_DIM_Title\")\n",
    "title_dups = analyze_duplicates(\n",
    "    dim_title, \n",
    "    \"gold_DIM_Title\", \n",
    "    [\"TitleKey\"],\n",
    "    \"Each title should have unique TitleKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "title_dups_business = analyze_duplicates(\n",
    "    dim_title, \n",
    "    \"gold_DIM_Title (Business Key)\", \n",
    "    [\"Tconst\"],\n",
    "    \"Each title should have unique Tconst (business key)\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 5. DIM_Genre\n",
    "dim_genre = spark.read.table(\"imdb_final_project.gold_DIM_Genre\")\n",
    "genre_dups = analyze_duplicates(\n",
    "    dim_genre, \n",
    "    \"gold_DIM_Genre\", \n",
    "    [\"GenreKey\"],\n",
    "    \"Each genre should have unique GenreKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "genre_dups_business = analyze_duplicates(\n",
    "    dim_genre, \n",
    "    \"gold_DIM_Genre (Business Key)\", \n",
    "    [\"GenreName\"],\n",
    "    \"Each genre should have unique GenreName (business key)\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 6. DIM_Profession\n",
    "dim_profession = spark.read.table(\"imdb_final_project.gold_DIM_Profession\")\n",
    "profession_dups = analyze_duplicates(\n",
    "    dim_profession, \n",
    "    \"gold_DIM_Profession\", \n",
    "    [\"ProfessionKey\"],\n",
    "    \"Each profession should have unique ProfessionKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "profession_dups_business = analyze_duplicates(\n",
    "    dim_profession, \n",
    "    \"gold_DIM_Profession (Business Key)\", \n",
    "    [\"Profession\"],\n",
    "    \"Each profession should have unique Profession (business key)\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 7. DIM_Crew\n",
    "dim_crew = spark.read.table(\"imdb_final_project.gold_DIM_Crew\")\n",
    "crew_dups = analyze_duplicates(\n",
    "    dim_crew, \n",
    "    \"gold_DIM_Crew\", \n",
    "    [\"CrewKey\"],\n",
    "    \"Each crew role should have unique CrewKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "crew_dups_business = analyze_duplicates(\n",
    "    dim_crew, \n",
    "    \"gold_DIM_Crew (Business Key)\", \n",
    "    [\"Crew_Role\"],\n",
    "    \"Each crew role should have unique Crew_Role (business key)\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 8. DIM_Principals\n",
    "dim_principals = spark.read.table(\"imdb_final_project.gold_DIM_Principals\")\n",
    "principals_dups = analyze_duplicates(\n",
    "    dim_principals, \n",
    "    \"gold_DIM_Principals\", \n",
    "    [\"PrincipalKey\"],\n",
    "    \"Each principal record should have unique PrincipalKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "principals_dups_business = analyze_duplicates(\n",
    "    dim_principals, \n",
    "    \"gold_DIM_Principals (Business Key)\", \n",
    "    [\"TitleKey\", \"NameKey\", \"ordering\"],\n",
    "    \"Each combination of TitleKey, NameKey, and ordering should be unique\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Fact Tables Duplicate Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 9. FACT_Title_Ratings\n",
    "fact_ratings = spark.read.table(\"imdb_final_project.gold_FACT_Title_Ratings\")\n",
    "ratings_dups = analyze_duplicates(\n",
    "    fact_ratings, \n",
    "    \"gold_FACT_Title_Ratings\", \n",
    "    [\"RatingKey\"],\n",
    "    \"Each rating record should have unique RatingKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "ratings_dups_business = analyze_duplicates(\n",
    "    fact_ratings, \n",
    "    \"gold_FACT_Title_Ratings (Business Key)\", \n",
    "    [\"TitleKey\"],\n",
    "    \"Each title should have only one rating record\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 10. FACT_Episodes\n",
    "fact_episodes = spark.read.table(\"imdb_final_project.gold_FACT_Episodes\")\n",
    "episodes_dups = analyze_duplicates(\n",
    "    fact_episodes, \n",
    "    \"gold_FACT_Episodes\", \n",
    "    [\"EpisodeKey\"],\n",
    "    \"Each episode record should have unique EpisodeKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "episodes_dups_business = analyze_duplicates(\n",
    "    fact_episodes, \n",
    "    \"gold_FACT_Episodes (Business Key)\", \n",
    "    [\"TitleKey\"],\n",
    "    \"Each episode title should appear only once\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Bridge Tables Duplicate Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 11. BRIDGE_TITLE_GENRE\n",
    "bridge_genre = spark.read.table(\"imdb_final_project.gold_BRIDGE_TITLE_GENRE\")\n",
    "bridge_genre_dups = analyze_duplicates(\n",
    "    bridge_genre, \n",
    "    \"gold_BRIDGE_TITLE_GENRE\", \n",
    "    [\"TitleGenreKey\"],\n",
    "    \"Each bridge record should have unique TitleGenreKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "bridge_genre_dups_business = analyze_duplicates(\n",
    "    bridge_genre, \n",
    "    \"gold_BRIDGE_TITLE_GENRE (Business Key)\", \n",
    "    [\"TitleKey\", \"GenreKey\"],\n",
    "    \"Each Title-Genre combination should be unique\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 12. BRIDGE_PROFESSION\n",
    "bridge_profession = spark.read.table(\"imdb_final_project.gold_BRIDGE_PROFESSION\")\n",
    "bridge_profession_dups = analyze_duplicates(\n",
    "    bridge_profession, \n",
    "    \"gold_BRIDGE_PROFESSION\", \n",
    "    [\"titleProfessionKey\"],\n",
    "    \"Each bridge record should have unique titleProfessionKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "bridge_profession_dups_business = analyze_duplicates(\n",
    "    bridge_profession, \n",
    "    \"gold_BRIDGE_PROFESSION (Business Key)\", \n",
    "    [\"NameKey\", \"ProfessionKey\"],\n",
    "    \"Each Name-Profession combination should be unique\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 13. Bridge_Title_Crew\n",
    "bridge_crew = spark.read.table(\"imdb_final_project.gold_Bridge_Title_Crew\")\n",
    "bridge_crew_dups = analyze_duplicates(\n",
    "    bridge_crew, \n",
    "    \"gold_Bridge_Title_Crew\", \n",
    "    [\"titleCrewKey\"],\n",
    "    \"Each bridge record should have unique titleCrewKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "bridge_crew_dups_business = analyze_duplicates(\n",
    "    bridge_crew, \n",
    "    \"gold_Bridge_Title_Crew (Business Key)\", \n",
    "    [\"TitleKey\", \"NameKey\", \"CrewKey\"],\n",
    "    \"Each Title-Name-Crew combination should be unique\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 14. BRIDGE_Akas\n",
    "bridge_akas = spark.read.table(\"imdb_final_project.gold_BRIDGE_Akas\")\n",
    "bridge_akas_dups = analyze_duplicates(\n",
    "    bridge_akas, \n",
    "    \"gold_BRIDGE_Akas\", \n",
    "    [\"TitleAkasKey\"],\n",
    "    \"Each akas record should have unique TitleAkasKey (surrogate key)\"\n",
    ")\n",
    "\n",
    "bridge_akas_dups_business = analyze_duplicates(\n",
    "    bridge_akas, \n",
    "    \"gold_BRIDGE_Akas (Business Key)\", \n",
    "    [\"TitleKey\", \"RegionKey\", \"LanguageKey\", \"AkasTitle\"],\n",
    "    \"Each Title-Region-Language-AkasTitle combination should be unique\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Consolidated Duplicate Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"CONSOLIDATED DUPLICATE ANALYSIS - ALL GOLD TABLES\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Collect all results\n",
    "all_duplicate_results = [\n",
    "    region_dups, region_dups_business,\n",
    "    language_dups, language_dups_business,\n",
    "    name_dups, name_dups_business,\n",
    "    title_dups, title_dups_business,\n",
    "    genre_dups, genre_dups_business,\n",
    "    profession_dups, profession_dups_business,\n",
    "    crew_dups, crew_dups_business,\n",
    "    principals_dups, principals_dups_business,\n",
    "    ratings_dups, ratings_dups_business,\n",
    "    episodes_dups, episodes_dups_business,\n",
    "    bridge_genre_dups, bridge_genre_dups_business,\n",
    "    bridge_profession_dups, bridge_profession_dups_business,\n",
    "    bridge_crew_dups, bridge_crew_dups_business,\n",
    "    bridge_akas_dups, bridge_akas_dups_business\n",
    "]\n",
    "\n",
    "# Filter out None values (from empty tables)\n",
    "all_duplicate_results = [r for r in all_duplicate_results if r is not None]\n",
    "\n",
    "# Create summary DataFrame\n",
    "duplicate_summary_df = spark.createDataFrame(all_duplicate_results)\n",
    "\n",
    "# Show tables with duplicates\n",
    "tables_with_duplicates = duplicate_summary_df.filter(col(\"duplicate_count\") > 0)\n",
    "\n",
    "if tables_with_duplicates.count() > 0:\n",
    "    print(\"\\n‚ö†Ô∏è  TABLES WITH DUPLICATES:\")\n",
    "    tables_with_duplicates.orderBy(desc(\"duplicate_percentage\")).show(50, truncate=False)\n",
    "else:\n",
    "    print(\"\\n‚úÖ NO DUPLICATES FOUND IN ANY TABLE!\")\n",
    "\n",
    "# Show overall summary\n",
    "print(\"\\nüìä OVERALL SUMMARY:\")\n",
    "duplicate_summary_df.orderBy(\"table_name\").show(50, truncate=False)\n",
    "\n",
    "# Statistics\n",
    "total_tables = duplicate_summary_df.count()\n",
    "tables_with_dups = tables_with_duplicates.count()\n",
    "total_rows = duplicate_summary_df.agg(_sum(\"total_rows\")).collect()[0][0]\n",
    "total_dup_rows = duplicate_summary_df.agg(_sum(\"total_duplicate_rows\")).collect()[0][0]\n",
    "\n",
    "print(f\"\\nüìà STATISTICS:\")\n",
    "print(f\"   Total tables analyzed: {total_tables}\")\n",
    "print(f\"   Tables with duplicates: {tables_with_dups}\")\n",
    "print(f\"   Tables without duplicates: {total_tables - tables_with_dups}\")\n",
    "print(f\"   Total rows across all tables: {total_rows:,}\")\n",
    "print(f\"   Total duplicate rows: {total_dup_rows:,}\")\n",
    "print(f\"   Overall duplicate rate: {(total_dup_rows / total_rows * 100):.2f}%\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Referential Integrity Checks\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"REFERENTIAL INTEGRITY CHECKS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Check if all foreign keys in bridge tables reference existing dimension keys\n",
    "\n",
    "# 1. BRIDGE_TITLE_GENRE referential integrity\n",
    "print(\"\\n1. Checking BRIDGE_TITLE_GENRE ‚Üí DIM_Title and DIM_Genre\")\n",
    "invalid_title_refs = bridge_genre.join(dim_title, bridge_genre.TitleKey == dim_title.TitleKey, \"left_anti\")\n",
    "invalid_genre_refs = bridge_genre.join(dim_genre, bridge_genre.GenreKey == dim_genre.GenreKey, \"left_anti\")\n",
    "print(f\"   Invalid TitleKey references: {invalid_title_refs.count()}\")\n",
    "print(f\"   Invalid GenreKey references: {invalid_genre_refs.count()}\")\n",
    "\n",
    "# 2. BRIDGE_PROFESSION referential integrity\n",
    "print(\"\\n2. Checking BRIDGE_PROFESSION ‚Üí DIM_NAME and DIM_Profession\")\n",
    "invalid_name_refs = bridge_profession.join(dim_name, bridge_profession.NameKey == dim_name.NameKey, \"left_anti\")\n",
    "invalid_prof_refs = bridge_profession.join(dim_profession, bridge_profession.ProfessionKey == dim_profession.ProfessionKey, \"left_anti\")\n",
    "print(f\"   Invalid NameKey references: {invalid_name_refs.count()}\")\n",
    "print(f\"   Invalid ProfessionKey references: {invalid_prof_refs.count()}\")\n",
    "\n",
    "# 3. Bridge_Title_Crew referential integrity\n",
    "print(\"\\n3. Checking Bridge_Title_Crew ‚Üí DIM_Title, DIM_NAME, and DIM_Crew\")\n",
    "invalid_crew_title_refs = bridge_crew.join(dim_title, bridge_crew.TitleKey == dim_title.TitleKey, \"left_anti\")\n",
    "invalid_crew_name_refs = bridge_crew.join(dim_name, bridge_crew.NameKey == dim_name.NameKey, \"left_anti\")\n",
    "invalid_crew_refs = bridge_crew.join(dim_crew, bridge_crew.CrewKey == dim_crew.CrewKey, \"left_anti\")\n",
    "print(f\"   Invalid TitleKey references: {invalid_crew_title_refs.count()}\")\n",
    "print(f\"   Invalid NameKey references: {invalid_crew_name_refs.count()}\")\n",
    "print(f\"   Invalid CrewKey references: {invalid_crew_refs.count()}\")\n",
    "\n",
    "# 4. BRIDGE_Akas referential integrity\n",
    "print(\"\\n4. Checking BRIDGE_Akas ‚Üí DIM_Title, DIM_Region, and DIM_Language\")\n",
    "# Note: -9999 is expected for missing regions/languages\n",
    "invalid_akas_title_refs = bridge_akas.join(dim_title, bridge_akas.TitleKey == dim_title.TitleKey, \"left_anti\")\n",
    "invalid_region_refs = bridge_akas.filter(col(\"RegionKey\") != -9999).join(dim_region, bridge_akas.RegionKey == dim_region.RegionKey, \"left_anti\")\n",
    "invalid_language_refs = bridge_akas.filter(col(\"LanguageKey\") != -9999).join(dim_language, bridge_akas.LanguageKey == dim_language.LanguageKey, \"left_anti\")\n",
    "print(f\"   Invalid TitleKey references: {invalid_akas_title_refs.count()}\")\n",
    "print(f\"   Invalid RegionKey references (excluding -9999): {invalid_region_refs.count()}\")\n",
    "print(f\"   Invalid LanguageKey references (excluding -9999): {invalid_language_refs.count()}\")\n",
    "\n",
    "# 5. FACT_Title_Ratings referential integrity\n",
    "print(\"\\n5. Checking FACT_Title_Ratings ‚Üí DIM_Title\")\n",
    "invalid_ratings_refs = fact_ratings.join(dim_title, fact_ratings.TitleKey == dim_title.TitleKey, \"left_anti\")\n",
    "print(f\"   Invalid TitleKey references: {invalid_ratings_refs.count()}\")\n",
    "\n",
    "# 6. FACT_Episodes referential integrity\n",
    "print(\"\\n6. Checking FACT_Episodes ‚Üí DIM_Title\")\n",
    "invalid_episodes_refs = fact_episodes.join(dim_title, fact_episodes.TitleKey == dim_title.TitleKey, \"left_anti\")\n",
    "print(f\"   Invalid TitleKey references: {invalid_episodes_refs.count()}\")\n",
    "\n",
    "# 7. DIM_Principals referential integrity\n",
    "print(\"\\n7. Checking DIM_Principals ‚Üí DIM_Title and DIM_NAME\")\n",
    "invalid_principals_title_refs = dim_principals.join(dim_title, dim_principals.TitleKey == dim_title.TitleKey, \"left_anti\")\n",
    "invalid_principals_name_refs = dim_principals.join(dim_name, dim_principals.NameKey == dim_name.NameKey, \"left_anti\")\n",
    "print(f\"   Invalid TitleKey references: {invalid_principals_title_refs.count()}\")\n",
    "print(f\"   Invalid NameKey references: {invalid_principals_name_refs.count()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Referential integrity checks complete!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"DUPLICATE ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d7d834f-c46e-43f1-aa2f-8918223312b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Entire row duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362a8445-bd11-4ed5-83e9-63c4cf06710d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Gold Layer - Duplicate Analysis (Excluding Primary Keys)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as _sum, desc, lit, md5, concat_ws, \n",
    "    coalesce, row_number  # ‚Üê ADD THESE\n",
    ")\n",
    "from pyspark.sql import Window\n",
    "\n",
    "def analyze_duplicates_excluding_pk(df, table_name, primary_key_column, description=\"\"):\n",
    "    \"\"\"\n",
    "    Comprehensive duplicate analysis - checks if entire rows are identical EXCEPT primary key\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to analyze\n",
    "    - table_name: Name of the table\n",
    "    - primary_key_column: Primary key column to EXCLUDE from duplicate check\n",
    "    - description: Optional description of the table\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "    \n",
    "    if total_rows == 0:\n",
    "        print(f\"‚ö†Ô∏è  {table_name} is EMPTY - skipping duplicate analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Get all columns except primary key\n",
    "    all_columns = [c for c in df.columns if c != primary_key_column]\n",
    "    \n",
    "    if len(all_columns) == 0:\n",
    "        print(f\"‚ö†Ô∏è  {table_name} has only primary key column - skipping\")\n",
    "        return None\n",
    "    \n",
    "    # Create a hash of all columns EXCEPT primary key\n",
    "    df_with_hash = df.withColumn(\n",
    "        \"row_hash\", \n",
    "        md5(concat_ws(\"||\", *[coalesce(col(c).cast(\"string\"), lit(\"NULL\")) for c in all_columns]))\n",
    "    )\n",
    "    \n",
    "    # Count duplicates based on hash (entire row except PK)\n",
    "    duplicate_check = (\n",
    "        df_with_hash\n",
    "        .groupBy(\"row_hash\")\n",
    "        .agg(count(\"*\").alias(\"occurrence_count\"))\n",
    "        .filter(col(\"occurrence_count\") > 1)\n",
    "    )\n",
    "    \n",
    "    duplicate_count = duplicate_check.count()\n",
    "    total_duplicate_rows = duplicate_check.agg(_sum(\"occurrence_count\")).collect()[0][0]\n",
    "    \n",
    "    if total_duplicate_rows:\n",
    "        total_duplicate_rows = int(total_duplicate_rows) - duplicate_count  # Subtract one occurrence per duplicate set\n",
    "    else:\n",
    "        total_duplicate_rows = 0\n",
    "    \n",
    "    duplicate_percentage = (total_duplicate_rows / total_rows * 100) if total_rows > 0 else 0\n",
    "    \n",
    "    print(\"=\" * 100)\n",
    "    print(f\"TABLE: {table_name}\")\n",
    "    print(f\"DESCRIPTION: {description}\")\n",
    "    print(f\"PRIMARY KEY (EXCLUDED): {primary_key_column}\")\n",
    "    print(f\"CHECKING: Row duplicates excluding primary key\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Total Rows: {total_rows:,}\")\n",
    "    print(f\"Unique Rows (excluding PK): {total_rows - total_duplicate_rows:,}\")\n",
    "    print(f\"Duplicate Sets: {duplicate_count:,}\")\n",
    "    print(f\"Extra Duplicate Rows: {total_duplicate_rows:,}\")\n",
    "    print(f\"Duplicate Percentage: {duplicate_percentage:.2f}%\")\n",
    "    \n",
    "    if duplicate_count > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: {duplicate_count} sets of duplicates found!\")\n",
    "        print(f\"    (Total {total_duplicate_rows} extra rows that are duplicates)\")\n",
    "        print(f\"    These rows have identical values in all columns EXCEPT {primary_key_column}\")\n",
    "        \n",
    "        # Show top duplicates by occurrence\n",
    "        print(\"\\nTop 10 duplicate sets by occurrence count:\")\n",
    "        duplicate_check.orderBy(desc(\"occurrence_count\")).show(10, truncate=False)\n",
    "        \n",
    "        # Show sample duplicate records with their primary keys\n",
    "        print(f\"\\nSample duplicate records (showing first 3 duplicate sets):\")\n",
    "        print(f\"Note: {primary_key_column} values are DIFFERENT, but all other columns are IDENTICAL\")\n",
    "        top_duplicate_hashes = duplicate_check.orderBy(desc(\"occurrence_count\")).limit(3).select(\"row_hash\").collect()\n",
    "        \n",
    "        for idx, row in enumerate(top_duplicate_hashes, 1):\n",
    "            hash_value = row[\"row_hash\"]\n",
    "            print(f\"\\n  Duplicate Set #{idx}:\")\n",
    "            duplicate_rows = df_with_hash.filter(col(\"row_hash\") == hash_value).drop(\"row_hash\")\n",
    "            duplicate_rows.show(10, truncate=False)\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No duplicates found - all records are unique (excluding {primary_key_column})!\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"table_name\": table_name,\n",
    "        \"primary_key\": primary_key_column,\n",
    "        \"total_rows\": total_rows,\n",
    "        \"unique_rows\": total_rows - total_duplicate_rows,\n",
    "        \"duplicate_sets\": duplicate_count,\n",
    "        \"duplicate_rows\": total_duplicate_rows,\n",
    "        \"duplicate_percentage\": round(duplicate_percentage, 2)\n",
    "    }\n",
    "\n",
    "print(\"Duplicate analysis function (excluding PK) loaded\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Dimension Tables Duplicate Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 1. DIM_Region\n",
    "dim_region = spark.read.table(\"imdb_final_project.gold_DIM_Region\")\n",
    "region_dups = analyze_duplicates_excluding_pk(\n",
    "    dim_region, \n",
    "    \"gold_DIM_Region\",\n",
    "    \"RegionKey\",\n",
    "    \"Region dimension with codes and descriptions\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 2. DIM_Language\n",
    "dim_language = spark.read.table(\"imdb_final_project.gold_DIM_Language\")\n",
    "language_dups = analyze_duplicates_excluding_pk(\n",
    "    dim_language, \n",
    "    \"gold_DIM_Language\",\n",
    "    \"LanguageKey\",\n",
    "    \"Language dimension with codes and descriptions\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 3. DIM_NAME\n",
    "dim_name = spark.read.table(\"imdb_final_project.gold_DIM_NAME\")\n",
    "name_dups = analyze_duplicates_excluding_pk(\n",
    "    dim_name, \n",
    "    \"gold_DIM_NAME\",\n",
    "    \"NameKey\",\n",
    "    \"Person dimension - actors, directors, writers\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 4. DIM_Title\n",
    "dim_title = spark.read.table(\"imdb_final_project.gold_DIM_Title\")\n",
    "title_dups = analyze_duplicates_excluding_pk(\n",
    "    dim_title, \n",
    "    \"gold_DIM_Title\",\n",
    "    \"TitleKey\",\n",
    "    \"Title dimension - movies, TV shows, episodes\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 5. DIM_Genre\n",
    "dim_genre = spark.read.table(\"imdb_final_project.gold_DIM_Genre\")\n",
    "genre_dups = analyze_duplicates_excluding_pk(\n",
    "    dim_genre, \n",
    "    \"gold_DIM_Genre\",\n",
    "    \"GenreKey\",\n",
    "    \"Genre dimension\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 6. DIM_Profession\n",
    "dim_profession = spark.read.table(\"imdb_final_project.gold_DIM_Profession\")\n",
    "profession_dups = analyze_duplicates_excluding_pk(\n",
    "    dim_profession, \n",
    "    \"gold_DIM_Profession\",\n",
    "    \"ProfessionKey\",\n",
    "    \"Profession dimension\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 7. DIM_Crew\n",
    "dim_crew = spark.read.table(\"imdb_final_project.gold_DIM_Crew\")\n",
    "crew_dups = analyze_duplicates_excluding_pk(\n",
    "    dim_crew, \n",
    "    \"gold_DIM_Crew\",\n",
    "    \"CrewKey\",\n",
    "    \"Crew role dimension (director, writer)\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 8. DIM_Principals\n",
    "dim_principals = spark.read.table(\"imdb_final_project.gold_DIM_Principals\")\n",
    "principals_dups = analyze_duplicates_excluding_pk(\n",
    "    dim_principals, \n",
    "    \"gold_DIM_Principals\",\n",
    "    \"PrincipalKey\",\n",
    "    \"Principal cast and crew assignments\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Fact Tables Duplicate Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 9. FACT_Title_Ratings\n",
    "fact_ratings = spark.read.table(\"imdb_final_project.gold_FACT_Title_Ratings\")\n",
    "ratings_dups = analyze_duplicates_excluding_pk(\n",
    "    fact_ratings, \n",
    "    \"gold_FACT_Title_Ratings\",\n",
    "    \"RatingKey\",\n",
    "    \"Title ratings and vote counts\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 10. FACT_Episodes\n",
    "fact_episodes = spark.read.table(\"imdb_final_project.gold_FACT_Episodes\")\n",
    "episodes_dups = analyze_duplicates_excluding_pk(\n",
    "    fact_episodes, \n",
    "    \"gold_FACT_Episodes\",\n",
    "    \"EpisodeKey\",\n",
    "    \"TV episode information\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Bridge Tables Duplicate Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 11. BRIDGE_TITLE_GENRE\n",
    "bridge_genre = spark.read.table(\"imdb_final_project.gold_BRIDGE_TITLE_GENRE\")\n",
    "bridge_genre_dups = analyze_duplicates_excluding_pk(\n",
    "    bridge_genre, \n",
    "    \"gold_BRIDGE_TITLE_GENRE\",\n",
    "    \"TitleGenreKey\",\n",
    "    \"Title to Genre many-to-many relationship\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 12. BRIDGE_PROFESSION\n",
    "bridge_profession = spark.read.table(\"imdb_final_project.gold_BRIDGE_PROFESSION\")\n",
    "bridge_profession_dups = analyze_duplicates_excluding_pk(\n",
    "    bridge_profession, \n",
    "    \"gold_BRIDGE_PROFESSION\",\n",
    "    \"titleProfessionKey\",\n",
    "    \"Person to Profession many-to-many relationship\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 13. Bridge_Title_Crew\n",
    "bridge_crew = spark.read.table(\"imdb_final_project.gold_Bridge_Title_Crew\")\n",
    "bridge_crew_dups = analyze_duplicates_excluding_pk(\n",
    "    bridge_crew, \n",
    "    \"gold_Bridge_Title_Crew\",\n",
    "    \"titleCrewKey\",\n",
    "    \"Title to Crew member many-to-many relationship\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# 14. BRIDGE_Akas\n",
    "bridge_akas = spark.read.table(\"imdb_final_project.gold_BRIDGE_Akas\")\n",
    "bridge_akas_dups = analyze_duplicates_excluding_pk(\n",
    "    bridge_akas, \n",
    "    \"gold_BRIDGE_Akas\",\n",
    "    \"TitleAkasKey\",\n",
    "    \"Title alternate names with regional variations\"\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Consolidated Duplicate Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"CONSOLIDATED DUPLICATE ANALYSIS - ALL GOLD TABLES\")\n",
    "print(\"(Excluding Primary Keys from Comparison)\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# Collect all results\n",
    "all_duplicate_results = [\n",
    "    region_dups,\n",
    "    language_dups,\n",
    "    name_dups,\n",
    "    title_dups,\n",
    "    genre_dups,\n",
    "    profession_dups,\n",
    "    crew_dups,\n",
    "    principals_dups,\n",
    "    ratings_dups,\n",
    "    episodes_dups,\n",
    "    bridge_genre_dups,\n",
    "    bridge_profession_dups,\n",
    "    bridge_crew_dups,\n",
    "    bridge_akas_dups\n",
    "]\n",
    "\n",
    "# Filter out None values (from empty tables)\n",
    "all_duplicate_results = [r for r in all_duplicate_results if r is not None]\n",
    "\n",
    "# Create summary DataFrame\n",
    "duplicate_summary_df = spark.createDataFrame(all_duplicate_results)\n",
    "\n",
    "# Show tables with duplicates\n",
    "tables_with_duplicates = duplicate_summary_df.filter(col(\"duplicate_rows\") > 0)\n",
    "\n",
    "if tables_with_duplicates.count() > 0:\n",
    "    print(\"\\n‚ö†Ô∏è  TABLES WITH DUPLICATES (Excluding Primary Keys):\")\n",
    "    tables_with_duplicates.orderBy(desc(\"duplicate_percentage\")).show(50, truncate=False)\n",
    "    \n",
    "    print(\"\\nüí° INTERPRETATION:\")\n",
    "    print(\"   These tables have rows where ALL columns are identical EXCEPT the primary key.\")\n",
    "    print(\"   This indicates potential data quality issues that should be investigated.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ NO DUPLICATES FOUND IN ANY TABLE!\")\n",
    "\n",
    "# Show overall summary\n",
    "print(\"\\nüìä OVERALL SUMMARY:\")\n",
    "duplicate_summary_df.select(\n",
    "    \"table_name\",\n",
    "    \"primary_key\",\n",
    "    \"total_rows\",\n",
    "    \"unique_rows\",\n",
    "    \"duplicate_sets\",\n",
    "    \"duplicate_rows\",\n",
    "    \"duplicate_percentage\"\n",
    ").orderBy(\"table_name\").show(50, truncate=False)\n",
    "\n",
    "# Statistics\n",
    "total_tables = duplicate_summary_df.count()\n",
    "tables_with_dups = tables_with_duplicates.count()\n",
    "total_rows = duplicate_summary_df.agg(_sum(\"total_rows\")).collect()[0][0]\n",
    "total_dup_rows = duplicate_summary_df.agg(_sum(\"duplicate_rows\")).collect()[0][0]\n",
    "total_unique_rows = duplicate_summary_df.agg(_sum(\"unique_rows\")).collect()[0][0]\n",
    "\n",
    "print(f\"\\nüìà STATISTICS:\")\n",
    "print(f\"   Total tables analyzed: {total_tables}\")\n",
    "print(f\"   Tables with duplicates: {tables_with_dups}\")\n",
    "print(f\"   Tables without duplicates: {total_tables - tables_with_dups}\")\n",
    "print(f\"   Total rows across all tables: {total_rows:,}\")\n",
    "print(f\"   Total unique rows: {total_unique_rows:,}\")\n",
    "print(f\"   Total duplicate rows: {total_dup_rows:,}\")\n",
    "print(f\"   Overall duplicate rate: {(total_dup_rows / total_rows * 100):.2f}%\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Deduplication Function (If Needed)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"DUPLICATE ANALYSIS COMPLETE!\")\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe65d23-0173-4c6d-9c3c-c202eafb7f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ## Pre-Deduplication Safety Check - Referential Integrity Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, count, countDistinct\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"REFERENTIAL INTEGRITY SAFETY CHECK\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 1. Check Impact of Deduplicating gold_BRIDGE_Akas\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n1. ANALYZING: gold_BRIDGE_Akas\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "bridge_akas = spark.read.table(\"imdb_final_project.gold_BRIDGE_Akas\")\n",
    "\n",
    "print(f\"Current total rows: {bridge_akas.count():,}\")\n",
    "print(f\"Duplicates to remove: ~10,260\")\n",
    "print(f\"Expected after dedup: ~{bridge_akas.count() - 10260:,}\")\n",
    "\n",
    "# Check if TitleAkasKey is used as FK anywhere (unlikely for bridge tables)\n",
    "print(\"\\n‚ö†Ô∏è  BRIDGE TABLES typically don't have other tables referencing them\")\n",
    "print(\"   They only reference dimension tables via their FKs\")\n",
    "print(\"   ‚úÖ SAFE TO DEDUPLICATE\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 2. Check Impact of Deduplicating gold_BRIDGE_PROFESSION\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n2. ANALYZING: gold_BRIDGE_PROFESSION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "bridge_profession = spark.read.table(\"imdb_final_project.gold_BRIDGE_PROFESSION\")\n",
    "\n",
    "print(f\"Current total rows: {bridge_profession.count():,}\")\n",
    "print(f\"Duplicates to remove: ~20\")\n",
    "print(f\"Expected after dedup: ~{bridge_profession.count() - 20:,}\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  BRIDGE TABLES typically don't have other tables referencing them\")\n",
    "print(\"   ‚úÖ SAFE TO DEDUPLICATE\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 3. Check Impact of Deduplicating gold_DIM_NAME (CRITICAL!)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n3. ANALYZING: gold_DIM_NAME - ‚ö†Ô∏è  DIMENSION TABLE (HIGH RISK)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "dim_name = spark.read.table(\"imdb_final_project.gold_DIM_NAME\")\n",
    "dim_principals = spark.read.table(\"imdb_final_project.gold_DIM_Principals\")\n",
    "bridge_profession = spark.read.table(\"imdb_final_project.gold_BRIDGE_PROFESSION\")\n",
    "bridge_crew = spark.read.table(\"imdb_final_project.gold_Bridge_Title_Crew\")\n",
    "\n",
    "print(f\"Current DIM_NAME rows: {dim_name.count():,}\")\n",
    "print(f\"Duplicates to remove: ~4\")\n",
    "\n",
    "# Check which NameKeys are duplicates\n",
    "dedup_columns = [\"NCONST\", \"PrimaryName\", \"BirthYear\", \"DeathYear\"]\n",
    "if \"ModifiedDate\" in dim_name.columns:\n",
    "    dedup_columns.append(\"ModifiedDate\")\n",
    "\n",
    "from pyspark.sql.functions import md5, concat_ws, coalesce, lit\n",
    "\n",
    "dim_name_with_hash = dim_name.withColumn(\n",
    "    \"row_hash\", \n",
    "    md5(concat_ws(\"||\", *[coalesce(col(c).cast(\"string\"), lit(\"NULL\")) for c in dedup_columns]))\n",
    ")\n",
    "\n",
    "duplicate_name_keys = (\n",
    "    dim_name_with_hash\n",
    "    .groupBy(\"row_hash\")\n",
    "    .agg(count(\"*\").alias(\"dup_count\"))\n",
    "    .filter(col(\"dup_count\") > 1)\n",
    "    .join(dim_name_with_hash, \"row_hash\")\n",
    "    .select(\"NameKey\", \"NCONST\", \"PrimaryName\")\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Duplicate NameKeys that will be removed:\")\n",
    "duplicate_name_keys.show(20, truncate=False)\n",
    "\n",
    "# Check if these NameKeys are referenced\n",
    "duplicate_keys_list = [row.NameKey for row in duplicate_name_keys.select(\"NameKey\").distinct().collect()]\n",
    "\n",
    "print(f\"\\nüìä Checking references to duplicate NameKeys:\")\n",
    "print(f\"   - Total duplicate NameKeys: {len(duplicate_keys_list)}\")\n",
    "\n",
    "if len(duplicate_keys_list) > 0:\n",
    "    # Check DIM_Principals\n",
    "    principals_refs = dim_principals.filter(col(\"NameKey\").isin(duplicate_keys_list)).count()\n",
    "    print(f\"   - Referenced in DIM_Principals: {principals_refs}\")\n",
    "    \n",
    "    # Check BRIDGE_PROFESSION\n",
    "    profession_refs = bridge_profession.filter(col(\"NameKey\").isin(duplicate_keys_list)).count()\n",
    "    print(f\"   - Referenced in BRIDGE_PROFESSION: {profession_refs}\")\n",
    "    \n",
    "    # Check Bridge_Title_Crew\n",
    "    crew_refs = bridge_crew.filter(col(\"NameKey\").isin(duplicate_keys_list)).count()\n",
    "    print(f\"   - Referenced in Bridge_Title_Crew: {crew_refs}\")\n",
    "    \n",
    "    total_refs = principals_refs + profession_refs + crew_refs\n",
    "    \n",
    "    if total_refs > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: {total_refs} rows in other tables reference duplicate NameKeys!\")\n",
    "        print(\"   ‚ùå NOT SAFE TO DEDUPLICATE without updating references!\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No other tables reference these duplicate NameKeys\")\n",
    "        print(\"   ‚úÖ SAFE TO DEDUPLICATE\")\n",
    "else:\n",
    "    print(\"‚úÖ SAFE TO DEDUPLICATE\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 4. Check Impact of Deduplicating gold_DIM_Region (CRITICAL!)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n4. ANALYZING: gold_DIM_Region - ‚ö†Ô∏è  DIMENSION TABLE (HIGH RISK)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "dim_region = spark.read.table(\"imdb_final_project.gold_DIM_Region\")\n",
    "bridge_akas = spark.read.table(\"imdb_final_project.gold_BRIDGE_Akas\")\n",
    "\n",
    "print(f\"Current DIM_Region rows: {dim_region.count():,}\")\n",
    "print(f\"Duplicates to remove: ~1\")\n",
    "\n",
    "# Check which RegionKeys are duplicates\n",
    "dedup_columns = [\"RegionCode\", \"RegionDescription\"]\n",
    "\n",
    "dim_region_with_hash = dim_region.withColumn(\n",
    "    \"row_hash\", \n",
    "    md5(concat_ws(\"||\", *[coalesce(col(c).cast(\"string\"), lit(\"NULL\")) for c in dedup_columns]))\n",
    ")\n",
    "\n",
    "duplicate_region_keys = (\n",
    "    dim_region_with_hash\n",
    "    .groupBy(\"row_hash\")\n",
    "    .agg(count(\"*\").alias(\"dup_count\"))\n",
    "    .filter(col(\"dup_count\") > 1)\n",
    "    .join(dim_region_with_hash, \"row_hash\")\n",
    "    .select(\"RegionKey\", \"RegionCode\", \"RegionDescription\")\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Duplicate RegionKeys that will be removed:\")\n",
    "duplicate_region_keys.show(20, truncate=False)\n",
    "\n",
    "# Check if these RegionKeys are referenced\n",
    "duplicate_region_keys_list = [row.RegionKey for row in duplicate_region_keys.select(\"RegionKey\").distinct().collect()]\n",
    "\n",
    "print(f\"\\nüìä Checking references to duplicate RegionKeys:\")\n",
    "print(f\"   - Total duplicate RegionKeys: {len(duplicate_region_keys_list)}\")\n",
    "\n",
    "if len(duplicate_region_keys_list) > 0:\n",
    "    # Check BRIDGE_Akas (excluding -9999)\n",
    "    akas_refs = bridge_akas.filter(\n",
    "        (col(\"RegionKey\").isin(duplicate_region_keys_list)) & \n",
    "        (col(\"RegionKey\") != -9999)\n",
    "    ).count()\n",
    "    print(f\"   - Referenced in BRIDGE_Akas: {akas_refs}\")\n",
    "    \n",
    "    if akas_refs > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: {akas_refs} rows in BRIDGE_Akas reference duplicate RegionKeys!\")\n",
    "        print(\"   ‚ùå NOT SAFE TO DEDUPLICATE without updating references!\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No other tables reference these duplicate RegionKeys\")\n",
    "        print(\"   ‚úÖ SAFE TO DEDUPLICATE\")\n",
    "else:\n",
    "    print(\"‚úÖ SAFE TO DEDUPLICATE\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### 5. Check Impact of Deduplicating gold_DIM_Title (CRITICAL!)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n5. ANALYZING: gold_DIM_Title - ‚ö†Ô∏è  DIMENSION TABLE (HIGHEST RISK)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "dim_title = spark.read.table(\"imdb_final_project.gold_DIM_Title\")\n",
    "dim_principals = spark.read.table(\"imdb_final_project.gold_DIM_Principals\")\n",
    "fact_ratings = spark.read.table(\"imdb_final_project.gold_FACT_Title_Ratings\")\n",
    "fact_episodes = spark.read.table(\"imdb_final_project.gold_FACT_Episodes\")\n",
    "bridge_genre = spark.read.table(\"imdb_final_project.gold_BRIDGE_TITLE_GENRE\")\n",
    "bridge_crew = spark.read.table(\"imdb_final_project.gold_Bridge_Title_Crew\")\n",
    "bridge_akas = spark.read.table(\"imdb_final_project.gold_BRIDGE_Akas\")\n",
    "\n",
    "print(f\"Current DIM_Title rows: {dim_title.count():,}\")\n",
    "print(f\"Duplicates to remove: ~1\")\n",
    "\n",
    "# Check which TitleKeys are duplicates\n",
    "dedup_columns = [\"Tconst\", \"TitleType\", \"PrimaryTitle\", \"OriginalTitle\", \n",
    "                 \"IsAdult\", \"ReleaseYear\", \"RuntimeMinutes\"]\n",
    "\n",
    "# Add SCD columns if they exist\n",
    "scd_columns = [\"EffectiveDate\", \"EndDate\", \"IsCurrent\", \"CreatedDate\", \"ModifiedDate\"]\n",
    "for scd_col in scd_columns:\n",
    "    if scd_col in dim_title.columns:\n",
    "        dedup_columns.append(scd_col)\n",
    "\n",
    "dim_title_with_hash = dim_title.withColumn(\n",
    "    \"row_hash\", \n",
    "    md5(concat_ws(\"||\", *[coalesce(col(c).cast(\"string\"), lit(\"NULL\")) for c in dedup_columns]))\n",
    ")\n",
    "\n",
    "duplicate_title_keys = (\n",
    "    dim_title_with_hash\n",
    "    .groupBy(\"row_hash\")\n",
    "    .agg(count(\"*\").alias(\"dup_count\"))\n",
    "    .filter(col(\"dup_count\") > 1)\n",
    "    .join(dim_title_with_hash, \"row_hash\")\n",
    "    .select(\"TitleKey\", \"Tconst\", \"PrimaryTitle\")\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Duplicate TitleKeys that will be removed:\")\n",
    "duplicate_title_keys.show(20, truncate=False)\n",
    "\n",
    "# Check if these TitleKeys are referenced\n",
    "duplicate_title_keys_list = [row.TitleKey for row in duplicate_title_keys.select(\"TitleKey\").distinct().collect()]\n",
    "\n",
    "print(f\"\\nüìä Checking references to duplicate TitleKeys:\")\n",
    "print(f\"   - Total duplicate TitleKeys: {len(duplicate_title_keys_list)}\")\n",
    "\n",
    "if len(duplicate_title_keys_list) > 0:\n",
    "    # Check all tables that reference TitleKey\n",
    "    principals_refs = dim_principals.filter(col(\"TitleKey\").isin(duplicate_title_keys_list)).count()\n",
    "    ratings_refs = fact_ratings.filter(col(\"TitleKey\").isin(duplicate_title_keys_list)).count()\n",
    "    episodes_refs = fact_episodes.filter(col(\"TitleKey\").isin(duplicate_title_keys_list)).count()\n",
    "    genre_refs = bridge_genre.filter(col(\"TitleKey\").isin(duplicate_title_keys_list)).count()\n",
    "    crew_refs = bridge_crew.filter(col(\"TitleKey\").isin(duplicate_title_keys_list)).count()\n",
    "    akas_refs = bridge_akas.filter(col(\"TitleKey\").isin(duplicate_title_keys_list)).count()\n",
    "    \n",
    "    print(f\"   - Referenced in DIM_Principals: {principals_refs}\")\n",
    "    print(f\"   - Referenced in FACT_Title_Ratings: {ratings_refs}\")\n",
    "    print(f\"   - Referenced in FACT_Episodes: {episodes_refs}\")\n",
    "    print(f\"   - Referenced in BRIDGE_TITLE_GENRE: {genre_refs}\")\n",
    "    print(f\"   - Referenced in Bridge_Title_Crew: {crew_refs}\")\n",
    "    print(f\"   - Referenced in BRIDGE_Akas: {akas_refs}\")\n",
    "    \n",
    "    total_refs = principals_refs + ratings_refs + episodes_refs + genre_refs + crew_refs + akas_refs\n",
    "    \n",
    "    if total_refs > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: {total_refs} rows across multiple tables reference duplicate TitleKeys!\")\n",
    "        print(\"   ‚ùå NOT SAFE TO DEDUPLICATE without updating references!\")\n",
    "        print(\"\\n   üîß SOLUTION: Must update all referencing tables to point to the kept TitleKey\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No other tables reference these duplicate TitleKeys\")\n",
    "        print(\"   ‚úÖ SAFE TO DEDUPLICATE\")\n",
    "else:\n",
    "    print(\"‚úÖ SAFE TO DEDUPLICATE\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Safety Check Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"SAFETY CHECK SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\"\"\n",
    "‚úÖ SAFE TO DEDUPLICATE (No References):\n",
    "   - gold_BRIDGE_Akas (bridge table - not referenced by others)\n",
    "   - gold_BRIDGE_PROFESSION (bridge table - not referenced by others)\n",
    "\n",
    "‚ö†Ô∏è  REQUIRES CAREFUL HANDLING (Dimension Tables):\n",
    "   - gold_DIM_NAME (may be referenced by Principals, Profession bridge, Crew bridge)\n",
    "   - gold_DIM_Region (may be referenced by BRIDGE_Akas)\n",
    "   - gold_DIM_Title (referenced by MANY tables - highest risk!)\n",
    "\n",
    "üìã RECOMMENDATION:\n",
    "   1. Run the checks above to see if duplicate keys are actually referenced\n",
    "   2. If NO references found ‚Üí Safe to deduplicate\n",
    "   3. If references found ‚Üí Need to update FK references before deduplication\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8290940688161255,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "IMDB_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
